{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Data Set\n",
    "\n",
    "The iris dataset is one of the traditional dummy datasets used for training. Contained data has been collected in the early $20$-th century regarding characteristics of three different types of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` library contains a module enabling easy loading of a number of standard datasets. This sub-module is simply the `datasets`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the following notebook we'll be going through the different steps one goes through when trying to figure out when studying a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Going through main features of data set.\n",
    "\n",
    "How is the `iris` object stored? What are its attributes? What do they correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why not represent data set in a human readable fasion?\n",
    "\n",
    "Represent dataset as a `pandas` data frame with names features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is it exactly that we want to do?\n",
    "\n",
    "It's crucial to be able to state our goals. Depending on the task we're looking into we're going to possibly build-up rather different work-flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Isolate a test set.\n",
    "\n",
    "In order to be able to compare two models' *accuracies* one shall need a common testing base ; that's the test set. The leftover of the `iris` dataset is the training set used to tune parameters of the model we've chosen to accomplish our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why not plot data when plotting makes sense?\n",
    "\n",
    "Look into the `matplotlib` documentation and releveant notebook within the course's repository for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Choose a model (simple first!) and train it.\n",
    "\n",
    "Once the assigned task is well defined you have a series of available models within the `scikit-learn` stack answering such task. Choose a simple one to start out a training process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate model\n",
    "\n",
    "Evaluating a model is firstly to enable comparison to other model's. It does only poorly make sense to evaluate a single model, unless you're overly satisfied with results. Part of the job is to choose among different models that's what evaluation is there for.\n",
    "\n",
    "There are a few number of steps to follow when evaluating a model:\n",
    "- Choose metric.\n",
    "    What is important ; proportion of right answers, proportions of wrong answers with respect to right ones? etc.\n",
    "- Check for score on test set.\n",
    "- Check for stability of scores through cross-validation. \n",
    "    Cutting training set into a number of statistically equivalent data sets one can train model on all chunks but one \n",
    "    and evaluating mode on leftover chunk. Repeating this strategy for each single chunk one gets as many scores as \n",
    "    chunks. This gives an idea of stability of score of model.\n",
    "- Plot learning curves to get an idea of overfitting or underfitting behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Get back to point 6.\n",
    "\n",
    "If score is not satisfying yet, assuming you're underfitting your data go on for a more complex model, if you're overfitting data go on for a more constrained or simpler one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
